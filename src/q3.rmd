---
title: "q3"
output: html_document
bibliography: bibliography.ris
csl: ../build/harvard-university-of-westminster.csl
nocite: | 
  @doc:5a52ed11e4b0e3e5a6364700, @doc:5a52ed04e4b0e3e5a63646ff, @doc:5a52ebfbe4b01d3dd55667a6, @doc:5a52e013e4b02942103f7fc1, @doc:5a52de2be4b01d3dd55666a1, @doc:5a52ddf0e4b0e3e5a63645e4, @doc:5a52ddd8e4b02942103f7f9c, @doc:5a52dd8ce4b0eeb35a4a1e8c, @doc:5a52dd7be4b08e15c00cdc97, @doc:5a52dc89e4b02942103f7f82, @doc:5a52dc77e4b02942103f7f81, @doc:5a52ca43e4b02942103f7d57, @doc:5a52c93ce4b02942103f7d43, @doc:5a5192fbe4b02942103f2d47, @doc:5a4f0c37e4b09d5cea02c23b, @doc:5a4edbf5e4b0ac315a55bfc7, @doc:5a4ec6f3e4b020410252c182, @doc:5a4ec56ae4b0062b162abbf0, @doc:5a4ec538e4b0ac315a55beae, @doc:5a4eba0ee4b020410252c08d, @doc:5a4e5a3ae4b09d5cea02a9a9, @doc:5a4db356e4b0ac315a5583fc, @doc:5a4db301e4b0c2a8b20cd934, @doc:5a4db27ee4b0c2a8b20cd927, @doc:5a4da870e4b0c2a8b20cd807, @doc:5a4c434ae4b0204102524624, @doc:5a4c3ba6e4b0ac315a552aac, @doc:5a4c3b69e4b0ac315a552aa8, @doc:5a4c395ce4b0c2a8b20c9406, @doc:5a4c2612e4b01d3b9773f835, @doc:5a4c1f06e4b0c2a8b20c917c, @doc:5a4bd8d3e4b09d5cea022500, @doc:5a4bd7bbe4b0062b162a30a0, @doc:5a4b8c54e4b09d5cea020a62, @doc:5a4b8c05e4b0c2a8b20c5456, @doc:5a4b82e4e4b0062b162a126c, @doc:5a458a17e4b022cc4f774122, @doc:5a458724e4b000cfec287c20, @doc:5a4582c4e4b02eb8a444a227, @doc:5a458258e4b022cc4f773fdd, @doc:5a457f22e4b022cc4f773f51, @doc:5a457cf1e4b06a7ca331608d
---

```{r setup-t3, include=FALSE, message=FALSE}
#knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
knitr::opts_chunk$set(echo = TRUE)
set.seed(4321)
```

```{r deps-t3, echo=FALSE, include=FALSE, message=FALSE}
#if(!require(doMC)) install.packages('doMC')
if(!require(rJava)) install.packages('rJava')
if(!require(caret)) install.packages("caret", repos = mirrorUrl)
if(!require(RWeka)) install.packages("RWeka", repos = mirrorUrl)
#e1071
if(!require(e1071)) install.packages("e1071", repos = mirrorUrl)
#libcoin
if(!require(coin)) install.packages("coin", repos = mirrorUrl)
#party
if(!require(party)) install.packages("party", repos = mirrorUrl)
#partykit
if(!require(partykit)) install.packages("partykit", repos = mirrorUrl)
#rpart
if(!require(rpart.plot)) install.packages("rpart.plot", repos = mirrorUrl)
#randomForest
if(!require(randomForest)) install.packages("randomForest", repos = mirrorUrl)
#ranger
if(!require(ranger)) install.packages("ranger", repos = mirrorUrl)
#
# configure multicore
#library("doMC")
library("e1071")
library("caret")
library("RWeka")
library("rpart.plot")
library("libcoin")
library("party")
library("partykit")
library("ranger")
library("randomForest")
```

```{r setup-t3_2, include=FALSE, message=FALSE}
#registerDoMC()
```
<section>

#Task 3: Build Train and Test a Decision Tree type Classifier

##Premise

>You need to construct, train and test Decision Tree type classifier (C4.5, Random Forest) in R. Train and test your decision tree classifier using the training and test sets generated based on the methods tried as part of the 2nd Task.

<!--
• Building of Decision Tree type classifier (C4.5, Random Forest) in R 8
• Training of Decision Tree type classifier (C4.5, Random Forest) in R 8
• Testing of Decision Tree type classifier (C4.5, Random Forest) 9
-->

##Decision trees

Decision Trees are human-friendly models because they are able to expose their logic easily in a visual way [@doc:5a4c434ae4b0204102524624, @doc:5a4c3b69e4b0ac315a552aa8], instead of being considered 'black boxes'; in addition to this, once trained they are quick to process new data as the rules of classification have already been definitively realised.

##C4.5 Decision Tree

Finding the C4.5 method as an existing library within R, brought up more than one option, both appear to use an open-source equivalent of C4.5 rather than an official C4.5 implementation; other options included using C5.0 which apparently supersedes C4.5; seeing as the task was to investigate C4.5, J48 has been chosen as a more faithful example of the algorithm.

Below is not only an investigation into C4.5 but also, a comparison of two variations. Initially all available dimensions will be used for training.

###The J48 method

```{r dt-c4_5_j48-h}
load("holdout.training.rda")
load("holdout.testing.rda")

dt.c4_5_j48_h <- J48(as.factor(rings)~., holdout.training) 
dt_sum.c4_5_j48_h <- summary(dt.c4_5_j48_h)
dt.c4_5_j48_h_party <- as.party(dt.c4_5_j48_h)
```

The J48 function is extremely quick with the dataset, training takes less than a second, which on its own is not necessarily worthy of note but certainly more interesting when compared to the speed of using the caret train method with a "J48" method argument value.

It is possible to look at the complexity of the tree by looking at the dimensionality, where the length is effectively the tree size, width is the number of leaves, or terminal nodes and the depth is effectively the number of conditional branch layers.

```{r dt-c4_5_j48-h_dimensions}
length(dt.c4_5_j48_h_party)
width(dt.c4_5_j48_h_party)
depth(dt.c4_5_j48_h_party)
```

The complexity of this tree is sufficiently complex to render a graphical representation useless. In fact the tree is so complex, it is time intensive as well as being of no value.

###The caret train J48 argument

```{r dt-c4_5_train-h}
dt.c4_5_h2 <- train(as.factor(rings) ~., method="J48", holdout.training, tuneLength = 8)
dt_sum.c4_5_h2 <- summary(dt.c4_5_h2)
dt.c4_5_h2_finalModel <- dt.c4_5_h2$finalModel
dt_sum.c4_5_h2_final <- summary(dt.c4_5_h2_finalModel)
```

Whilst in comparisons to other forms of training and data-mining algorithms, under 5 minutes for a 4,000 by 9 dataset might seem okay, in comparison to the J48 function, there seems to be something at odds. The proof will be in the comparison of the two models with regard to accuracy on the test dataset.

Again, the the complexity of the tree can be seen through the dimensionality, length being total size, width being leaf nodes and depth being branch layer count. 

```{r dt-c4_5_train-h_dimensions}
#dt.c4_5_h2
#dt.c4_5_h2_finalModel <- dt.c4_5_h2$finalModel
#dt.c4_5_h2_finalModel
dt.c4_5_h2_finalModel_party <- as.party(dt.c4_5_h2_finalModel)
#dt.c4_5_h2_finalModel_party <- as.party(dt.c4_5_h2$finalModel)
length(dt.c4_5_h2_finalModel_party)
width(dt.c4_5_h2_finalModel_party)
depth(dt.c4_5_h2_finalModel_party)
```

It would appear that the 'final model' derived from the train function has already been pruned, making for a simpler decision tree; this tree is actually able to be represented graphically within a reasonable amount of time (under 10 seconds).

```{r c4_5-plot_tree}
plot(dt.c4_5_h2_finalModel_party)
```

Even though the tree is able to be rendered it is not easy to get anything meaningful out of this. Perhaps the most pertinent point is that not only could the classification levels benefit from being simplified but using fewer dimensions for observation would also force a simpler set of conditional branching.

###Comparing the two C4.5/J48 methods

Below follows the summaries from both methods, for examination: 

```{r c4_5-summaries}
kable(dt_sum.c4_5_j48_h$confusionMatrix)
kable(dt_sum.c4_5_h2$confusionMatrix)
```

####Accuracy comparison of C4.5/J48 models

Despite the values being present in the summaries, to clarify understanding, they are repeated below:

```{r c4_5-accuracy_comparison}
dt_sum.c4_5_j48_h$details[1]
dt_sum.c4_5_h2$details[1]
```

The J48 function has a significantly higher accuracy compared to the train function J48 call but at this stage it is hard to be confident this is a good thing; given the difference in tree complexity, it could well be that the J48 function suffers from massive over-fitting, while the train call has done some excessive pruning which has not only accounted for the extra time for the function to complete but also the diminished accuracy. To understand things further it's really necessary to test the trees against the validation subset.

####Comparing the two C4.5/J48 models after prediction

What follows is the output of testing the models against the test subset.

```{r dt-c4_5_j48_test}
holdout.test_rings <- holdout.testing$rings
dt.c4_5_j48_h_test <- predict(dt.c4_5_j48_h, newdata = holdout.testing)
holdout.test_levels <- min(holdout.test_rings):max(holdout.test_rings)
dt.c4_5_j48_h_test_cm <- confusionMatrix(factor(dt.c4_5_j48_h_test, levels=holdout.test_levels), factor(holdout.test_rings, levels = holdout.test_levels))
kable(dt.c4_5_j48_h_test_cm$table)
dt.c4_5_j48_h_test_cm$overall[1]
```

```{r dt-c4_5h2_test}
#dt.c4_5_h2
dt.c4_5_h2_test <- predict(dt.c4_5_h2, newdata = holdout.testing)
dt.c4_5_h2_test_cm <- confusionMatrix(factor(dt.c4_5_h2_test, levels=holdout.test_levels), factor(holdout.test_rings, levels = holdout.test_levels))
kable(dt.c4_5_h2_test_cm$table)
dt.c4_5_h2_test_cm$overall[1]
```

It would appear that despite the J48 function derived model claiming a higher accuracy, the resulting confusion matrix data suggests otherwise. Neither 23.6% nor 20.9% is particularly encouraging, however for the improvement in speed, the loss of about 12% in accuracy (approximately 3/24), might be reasonable if the overall accuracy can be improved in other ways (like further tuning). To investigate this further, the next steps are to look at reducing the number of attributes observed based on the earlier analysis of data, and  training the models to achieve simpler classification goals.

###C4.5 with refined dataset

To attempt attainment of high accuracy, it's worth looking to run the very same decision tree functions against a streamline classification aim, with a more targeted formula. Using raw rings and the classification levels amounted to nearly 30 possible outcomes, so bringing that down to a tiered factor of 3 age groups has to make things not only more performant but it's easier to be more accurate when the intended classification type has a larger scope.

####Refined formula

Previously, each of the attributes of a continuous numeric type were analysed for a correlation to the number of rings. To have a simpler decision tree it stands to reason that picking only the most relevant attributes for the problem of this particular classification are used. To this end, the attributes selected for the revised formula will be those top three correlates: _weight.shell, height, diameter_.

```{r tree-formula}
dt.formula <- as.formula(age_group ~ weight.shell + height + diameter)
```

####The J48 method with updated formula and simpler classification

```{r dt-c4_5_j48-h_a}
dt.c4_5_j48_h_a <- J48(dt.formula, holdout_age_groups.training) 
dt_sum.c4_5_j48_h_a <- summary(dt.c4_5_j48_h_a)
dt.c4_5_j48_h_a_party <- as.party(dt.c4_5_j48_h_a)
#
dt_sum.c4_5_j48_h_a
#
length(dt.c4_5_j48_h_a_party)
width(dt.c4_5_j48_h_a_party)
depth(dt.c4_5_j48_h_a_party)
#
plot(dt.c4_5_j48_h_a_party)
```

###The caret train J48 argument with updated formula and simpler classification

```{r dt-c4_5_train-h_a}
dt.c4_5_h2_a <- train(dt.formula, method="J48", holdout_age_groups.training, tuneLength = 8)
```

```{r dt-c4_5_train-h_a_results}
dt_sum.c4_5_h2_a <- summary(dt.c4_5_h2_a)
dt_sum.c4_5_h2_a_final <- summary(dt.c4_5_h2_a$finalModel)
#
dt_sum.c4_5_h2_a
#
dt.c4_5_h2_a_finalModel <- dt.c4_5_h2_a$finalModel
dt.c4_5_h2_a_finalModel_party <- as.party(dt.c4_5_h2_a_finalModel)
#
length(dt.c4_5_h2_a_finalModel_party)
width(dt.c4_5_h2_a_finalModel_party)
depth(dt.c4_5_h2_a_finalModel_party)
#
plot(dt.c4_5_h2_a_finalModel_party)
```

####Accuracy comparison of revised C4.5/J48 models

```{r c4_5-a-accuracy_comparison}
dt_sum.c4_5_j48_h_a$details[1]
dt_sum.c4_5_h2_a$details[1]
```

Interestingly the J48 function has produced an accuracy rating that is only marginally better than the previous version while the train J48 function call seems to have improved significantly so that it is nearly on a par with the J48 function result. Looking at the models after validation has happened will hopefully provide even more revealing findings.

####Comparing the revised C4.5/J48 model after prediction

What follows is the output of testing the models against the test subset.

```{r dt-c4_5_j48_h_a_test}
#holdout.test_rings <- holdout.testing$rings
dt.c4_5_j48_h_a_test <- predict(dt.c4_5_j48_h_a, newdata = holdout_age_groups.testing)
#holdout.test_levels <- min(holdout.test_rings):max(holdout.test_rings)
#dt.c4_5_j48_h_a_test_cm <- confusionMatrix(factor(dt.c4_5_j48_h_test, levels=holdout.test_levels), factor(holdout.test_rings, levels = holdout.test_levels))
dt.c4_5_j48_h_a_test_cm <- confusionMatrix(dt.c4_5_j48_h_a_test, holdout_age_groups.testing$age_group)
kable(dt.c4_5_j48_h_a_test_cm$table)
dt.c4_5_j48_h_a_test_cm$overall[1]
```

```{r dt-c4_5h2_a_test}
#dt.c4_5_h2
dt.c4_5_h2_a_test <- predict(dt.c4_5_h2_a, newdata = holdout_age_groups.testing)
dt.c4_5_h2_a_test_cm <- confusionMatrix(dt.c4_5_h2_a_test, holdout_age_groups.testing$age_group)
kable(dt.c4_5_h2_a_test_cm$table)
dt.c4_5_h2_a_test_cm$overall[1]
```

The accuracy has improved markedly so it's safe to say that the combination of streamlining the formula and creating a simpler classification requirement has improved things; this is the new baseline, now it's worth looking at any improvement that can be made through using the k-folds and leave-one-out cross-validation techniques.

###C4.5 with cross-validation techniques

Given how the accuracy between the two forms of C4.5 model generation narrowed to an absolute percentage delta of less than 1 percent, coupled with the speed at which  the J48 function returns, the next phase of experimentation will occur only with this function and the relevant training control options.

####J48 with k-folds training 

The J48 function does not accept the caret training control objects as valid control parameters; calling the Weka function `WOW` with `"J48"` as the sole argument presents the list of arguments that can be passed in to a `Weka_control` function call to configure training.

```{r j48-WOW}
WOW("J48")
```

Unfortunately, for the performant J48 method, none of these options seems to allow for custom methods of training. At this point, J48 has to be disregarded due to inflexibility despite such good run-time training speeds.

####Train with J48 method and k-folds training 

Three types of k-folds cross-validation configurations were created; at this point it's opportune to examine which, if any, are able to improve the accuracy of training and validation. Each of the three types of k-folds configurations will be used to train the decision tree.

The important implementation detail to emphasise here is that while the holdout method required a one-off explicit call to partition the data, with the user assigning which part as training or testing subset, with cross-validation the entire dataset is passed into the training call; this is because the training will occur several times using many subsampled testing subsets from the original sample. For the sake of completeness, the holdout testing subset can still be used to further interrogate the model, in order to better compare against other classifiers.

**K-folds, with 8 folds**

```{r dt-c4_5_j48-train-rkf_a}
dt.c4_5_j48_kf_a <- train(dt.formula, method="J48", abalone_data_cleansed_age_groups, tuneLength = 8, trControl = cv.train_control_8)
```

**K-folds, with 8 folds, 5 repetitions**

```{r dt-c4_5_j48-train-r5kf8_a}
dt.c4_5_j48_r5kf8_a <- train(dt.formula, method="J48", abalone_data_cleansed_age_groups, tuneLength = 8, trControl = cv.train_control_8_5)
```

**K-folds, with 13 folds, 5 repetitions**

```{r dt-c4_5_j48-train-r5kf13_a}
dt.c4_5_j48_r5kf13_a <- train(dt.formula, method="J48", abalone_data_cleansed_age_groups, tuneLength = 8, trControl = cv.train_control_13_5)
```

**K-folds, with 13 folds, 8 repetitions**

```{r dt-c4_5_j48-train-r8kf13_a}
dt.c4_5_j48_r8kf13_a <- train(dt.formula, method="J48", abalone_data_cleansed_age_groups, tuneLength = 8, trControl = cv.train_control_13_8)
```

#####Training results J48 method and k-folds training 

Below is a comparison of the different training results for k-folds; ultimately, the best one will be picked as the preferred use of k-folds going forward; obviously should this preferred configuration turn out to be too time intensive with other types of classifier, falling back to another configuration will be considered.

**K-folds, with 8 folds**

```{r dt-c4_5_train-fk_a_results}
dt_sum.c4_5_j48_kf_a <- summary(dt.c4_5_j48_kf_a)
dt.c4_5_j48_kf_a_final <- summary(dt.c4_5_j48_kf_a$finalModel)
#
dt_sum.c4_5_j48_kf_a
#
dt.c4_5_j48_kf_a_finalModel <- dt.c4_5_h2_a$finalModel
dt.c4_5_j48_kf_a_finalModel_party <- as.party(dt.c4_5_h2_a$finalModel)
#
length(dt.c4_5_j48_kf_a_finalModel_party)
width(dt.c4_5_j48_kf_a_finalModel_party)
depth(dt.c4_5_j48_kf_a_finalModel_party)
#
plot(dt.c4_5_j48_kf_a_finalModel_party)
```

**K-folds, with 8 folds, 5 repetitions**

```{r dt-c4_5_train-r5kf8_a_results}
dt_sum.c4_5_r5kf8_a <- summary(dt.c4_5_j48_r5kf8_a)
dt.c4_5_j48_r5kf8_a_final <- summary(dt.c4_5_j48_r5kf8_a$finalModel)
#
dt_sum.c4_5_r5kf8_a
#
dt.c4_5_j48_r5kf8_a_finalModel <- dt.c4_5_j48_r5kf8_a$finalModel
dt.c4_5_j48_r5kf8_a_finalModel_party <- as.party(dt.c4_5_j48_r5kf8_a_finalModel)
#
length(dt.c4_5_j48_r5kf8_a_finalModel_party)
width(dt.c4_5_j48_r5kf8_a_finalModel_party)
depth(dt.c4_5_j48_r5kf8_a_finalModel_party)
#
plot(dt.c4_5_j48_r5kf8_a_finalModel_party)
```

**K-folds, with 13 folds, 5 repetitions**

```{r dt-c4_5_train-r5kf13_a_results}
dt_sum.c4_5_r5kf13_a <- summary(dt.c4_5_j48_r5kf13_a)
dt.c4_5_j48_r5kf13_a_final <- summary(dt.c4_5_j48_r5kf13_a$finalModel)
#
dt_sum.c4_5_r5kf13_a
#
dt.c4_5_j48_r5kf13_a_finalModel <- dt.c4_5_j48_r5kf13_a$finalModel
dt.c4_5_j48_r5kf13_a_finalModel_party <- as.party(dt.c4_5_j48_r5kf13_a_finalModel)
#
length(dt.c4_5_j48_r5kf13_a_finalModel_party)
width(dt.c4_5_j48_r5kf13_a_finalModel_party)
depth(dt.c4_5_j48_r5kf13_a_finalModel_party)
#
plot(dt.c4_5_j48_r5kf13_a_finalModel_party)
```

**K-folds, with 13 folds, 8 repetitions**

```{r dt-c4_5_train-r8kf13_a_results}
dt_sum.c4_5_r8kf13_a <- summary(dt.c4_5_j48_r8kf13_a)
dt.c4_5_j48_r8kf13_a_final <- summary(dt.c4_5_j48_r8kf13_a$finalModel)
#
dt_sum.c4_5_r8kf13_a
#
dt.c4_5_j48_r8kf13_a_finalModel <- dt.c4_5_j48_r8kf13_a$finalModel
dt.c4_5_j48_r8kf13_a_finalModel_party <- as.party(dt.c4_5_j48_r8kf13_a_finalModel)
#
length(dt.c4_5_j48_r8kf13_a_finalModel_party)
width(dt.c4_5_j48_r8kf13_a_finalModel_party)
depth(dt.c4_5_j48_r8kf13_a_finalModel_party)
#
plot(dt.c4_5_j48_r8kf13_a_finalModel_party)
```


```{r c4_5_cv-pcCorrect}
dt_sum.c4_5_j48_kf_a$details[1]
dt_sum.c4_5_r5kf8_a$details[1]
dt_sum.c4_5_r5kf13_a$details[1]
```

After having run the training, the preliminary results suggest that while repeating the whole k-folds process a number of times can add some more accuracy, increasing the _k_ count doesn't add much more; depending on the size of the real-world dataset and the economic benefit of even the slightest improvement in accuracy, it's arguable that a trade-off between accuracy and performance can be made and sometimes a quicker less accurate option is the right tool for the job. That being said, prediction is significantly faster than training for decision trees; once the model has been built, new data is just processed into a class. 

The fact that for the k-folds with 5 repetitions, the \(k = 13\) version took about twice as long to compute as the \(k = 8\) version, the 8 version will be considered the preferred choice, assuming the validation doesn't suggest otherwise.

#####Testing results J48 method and k-folds training 

**K-folds, with 8 folds**

```{r dt-c4_5_j48_kf_a_test}
dt.c4_5_j48_kf_a_test <- predict(dt.c4_5_j48_kf_a, newdata = holdout_age_groups.testing)
dt.c4_5_j48_kf_a_test_cm <- confusionMatrix(dt.c4_5_j48_kf_a_test, holdout_age_groups.testing$age_group)
kable(dt.c4_5_j48_kf_a_test_cm$table)
dt.c4_5_j48_kf_a_test_cm$overall[1]
```

**K-folds, with 8 folds, 5 repetitions**

```{r dt-c4_5_j48_r5kf8_a_test}
dt.c4_5_j48_r5kf8_a_test <- predict(dt.c4_5_j48_r5kf8_a, newdata = holdout_age_groups.testing)
dt.c4_5_j48_r5kf8_a_test_prob <- predict(dt.c4_5_j48_r5kf8_a, newdata = holdout_age_groups.testing, type = "prob")
dt.c4_5_j48_r5kf8_a_test_cm <- confusionMatrix(dt.c4_5_j48_r5kf8_a_test, holdout_age_groups.testing$age_group)
kable(dt.c4_5_j48_r5kf8_a_test_cm$table)
dt.c4_5_j48_r5kf8_a_test_cm$overall[1]
```

**K-folds, with 13 folds, 5 repetitions**

```{r dt-c4_5_j48_r5kf13_a_test}
dt.c4_5_j48_r5kf13_a_test <- predict(dt.c4_5_j48_r5kf13_a, newdata = holdout_age_groups.testing)
dt.c4_5_j48_r5kf13_a_test_cm <- confusionMatrix(dt.c4_5_j48_r5kf13_a_test, holdout_age_groups.testing$age_group)
kable(dt.c4_5_j48_r5kf13_a_test_cm$table)
dt.c4_5_j48_r5kf13_a_test_cm$overall[1]
```

**K-folds, with 13 folds, 8 repetitions**

```{r dt-c4_5_j48_r8kf13_a_test}
dt.c4_5_j48_r8kf13_a_test <- predict(dt.c4_5_j48_r8kf13_a, newdata = holdout_age_groups.testing)
dt.c4_5_j48_r8kf13_a_test_prob <- predict(dt.c4_5_j48_r8kf13_a, newdata = holdout_age_groups.testing, type = "prob")
dt.c4_5_j48_r8kf13_a_test_cm <- confusionMatrix(dt.c4_5_j48_r8kf13_a_test, holdout_age_groups.testing$age_group)
kable(dt.c4_5_j48_r8kf13_a_test_cm$table)
dt.c4_5_j48_r8kf13_a_test_cm$overall[1]
```


####J48 with leave-one-out training 

The last C4.5 decision tree to explore is a model using the leave-one-out training method, however research [@doc:5a4eba0ee4b020410252c08d] and experimentation suggests this is too computationally expensive for decision trees; when attempting to train a tree with this training control, over an hour passed without any output; it would appear that for trees, not only _p_ but _n_ (the total number of observations) should also be very low, certainly smaller than the Abalone dataset. 


####C4.5 Conclusion

The results were lower than expected, with barely any improvement between the holdout method and the k-folds cross-validation technique; while further experimentation into increasing the number of steps might return higher accuracy rates, the cost of training goes up significantly. It will have to be seen later how well the results compare to all the rest.

With regard to the Abalone dataset in particular, it may not be the most suitable model for classification, even if cross-validation if able to improve training slightly.

##Random Forest Decision Tree

The Random Forest Decision Tree is an algorithm that creates multiple internalised decision trees [@doc:5a4ec56ae4b0062b162abbf0], which are all used when trying to classify new data. Each tree will vote, with a weight as to what the classification will be, with the final classification going to the greatest vote value. Random Forests are fast, less prone to over-fitting and work well with incomplete data. The major disadvantage of Random Forests is that they can't be used for regression, failing to extrapolate a result for data that is out-of-bounds of the original training dataset; given the scope of this particular Abalone age group classification, this shouldn't be too much of an issue.

With the previous Decision Tree (C4.5) training, much experimenting was done as to how the dataset should be prepared for the purpose of classification and the optimal training controls for Decision Trees; it seems reasonable enough to streamline the analysis of the Random Forest by carrying over into this sub-section the two main lessons learned: 

 - The classification problem should just aim to categorise the Abalone instances into 3 age groups
 - The k-folds technique benefits from repetition to more thoroughly train with the data but the number of folds needn't be too high.
 
For the Random Forest training, only one configuration of repeated k-folds will be used, namely '8 folds, 5 times' as it provided adequate improvement whereupon '13 folds, 5 times' barely improved.

###Random Forest Holdout

The same holdout values derived in _Task 2_  that were used for C4.5, will be applied for testing and training the Random Forest.

####Random Forest Holdout training

```{r dt-rf-h_a}
dt.rf_h_a <- train(dt.formula, data = holdout_age_groups.training, method = "rf", prox= TRUE)
```


```{r dt-rf-h_a-train_results}
dt.rf_h_a_finalModel <- dt.rf_h_a$finalModel
dt_sum.rf_h_a <- summary(dt.rf_h_a)
#
dt.rf_h_a_finalModel$forest[11]
dt.rf_h_a_finalModel$forest[12]
#
plot(dt.rf_h_a)
plot(dt.rf_h_a_finalModel)
```

After training, it's possible to observe there are 500 trees in the forest. Another observation is that even though no control was provided, the default caret control for random forest uses the Bootstrap method, this in effect reuses and recycles the holdout training data to train the forest against more data. As such, another library will be used to attempt to examine the results with a more strict holdout training of a Random Forest.

####Random Forest Holdout testing

```{r dt-rf-h_a-test_results}
dt.rf_h_a_test <- predict(dt.rf_h_a, newdata = holdout_age_groups.testing)

dt.rf_h_a_test_cm <- confusionMatrix(dt.rf_h_a_test, holdout_age_groups.testing$age_group)
kable(dt.rf_h_a_test_cm$table)
dt.rf_h_a_test_cm$overall[1]
```

####Another Random Forest Holdout

The `ranger` package has a method designed specifically for holdout Random Forests, which is used next to explore if there could be any better performance out of the traditional holdout technique. 

```{r dt-hrf-h_a}
dt.hrf_h_a <- holdoutRF(dt.formula, holdout_age_groups.training)
dt.hrf_h_a_prob <- holdoutRF(dt.formula, holdout_age_groups.training, probability = TRUE)
```

For some reason, not quite clear from the documentation, this method produces 2 Random Forests.

```{r dt-hrf-h_a-train_results}
#dt.hrf_h_a_finalModel <- dt.hrf_h_a$finalModel
dt_sum.hrf_h_a <- summary(dt.hrf_h_a)
#
dt.hrf_h_a$rf1
dt.hrf_h_a$rf2
```

```{r dt-hrf-h_a-test_results}
dt.hrf1_h_a_test <- predict(dt.hrf_h_a$rf1, holdout_age_groups.testing)
dt.hrf1_h_a_test_prob <- predict(dt.hrf_h_a_prob$rf1, holdout_age_groups.testing)

dt.hrf1_h_a_test_cm <- confusionMatrix(dt.hrf1_h_a_test$predictions, holdout_age_groups.testing$age_group)
kable(dt.hrf1_h_a_test_cm$table)
dt.hrf1_h_a_test_cm$overall[1]
#
dt.hrf2_h_a_test <- predict(dt.hrf_h_a$rf2, holdout_age_groups.testing)
dt.hrf2_h_a_test_prob <- predict(dt.hrf_h_a_prob$rf2, holdout_age_groups.testing)

dt.hrf2_h_a_test_cm <- confusionMatrix(dt.hrf2_h_a_test$predictions, holdout_age_groups.testing$age_group)
kable(dt.hrf2_h_a_test_cm$table)
dt.hrf2_h_a_test_cm$overall[1]
```

These results are comparable to the caret bootstrap results, to the extent that while the caret package Random Forest call has a lower accuracy value, the fact it correctly classifies more old Abalone instances needs to be considered in its favour when compared to the first _(rf1)_ Random Forest returns a slightly better accuracy rate.

###Random Forest Cross-validation

Even though opinion in the Data Science community is that a Random Forest uses techniques that effectively provide internal solutions for cross-validation while training, setting an explcit control will ensure a complete rotation of data for testing and training.

####Random Forest Cross-validation training

```{r dt-rf-cv-r5kf8_a}
dt.rf_cv_r5kf8_a <- train(dt.formula, data = abalone_data_cleansed_age_groups, method = "rf", prox= TRUE, trControl = cv.train_control_8_5)
```

```{r dt-rf-cv-r5kf8_a-train_results}
dt.rf_cv_r5kf8_a_finalModel <- dt.rf_cv_r5kf8_a$finalModel
dt_sum.rf_cv_r5kf8_a <- summary(dt.rf_cv_r5kf8_a)
#
dt.rf_cv_r5kf8_a_finalModel$forest[11]
dt.rf_cv_r5kf8_a_finalModel$forest[12]
#
plot(dt.rf_cv_r5kf8_a)
plot(dt.rf_cv_r5kf8_a_finalModel)
```

####Random Forest Cross-validation testing

```{r dt-rf-r5kf8_a-test_results}
dt.rf_cv_r5kf8_a_test <- predict(dt.rf_cv_r5kf8_a, newdata = holdout_age_groups.testing)
dt.rf_cv_r5kf8_a_test_prob <- predict(dt.rf_cv_r5kf8_a, newdata = holdout_age_groups.testing, type = "prob")

dt.rf_cv_r5kf8_a_test_cm <- confusionMatrix(dt.rf_cv_r5kf8_a_test, holdout_age_groups.testing$age_group)
kable(dt.rf_cv_r5kf8_a_test_cm$table)
dt.rf_cv_r5kf8_a_test_cm$overall[1]
```

**Out of bag predictions**

```{r dt-rf-r5kf8_a-test2_results}
dt.rf_cv_r5kf8_a_test2 <- predict(dt.rf_cv_r5kf8_a)
dt.rf_cv_r5kf8_a_test2_prob <- predict(dt.rf_cv_r5kf8_a, newdata = holdout_age_groups.testing, type = "prob")

dt.rf_cv_r5kf8_a_test2_cm <- confusionMatrix(dt.rf_cv_r5kf8_a_test2, abalone_data_cleansed_age_groups$age_group)
kable(dt.rf_cv_r5kf8_a_test2_cm$table)
dt.rf_cv_r5kf8_a_test2_cm$overall[1]
```

While there appears to be little difference between the graphs depicting the nature of the two caret Random Forests, this second example using cross-validation has done a near perfect classification of the Abalone dataset by age grouping. While this seems like a useful result, it could well be that this is a result of incorrectly using the entire sample set with the Random Forest and assuming cross-validation would not cause overfitting. Consultation with a reliable source will be required to confirm this one way or another. With that in mind, yet another Random Forest will be trained using cross-validation but only with the holdout training data.

###Random Forest Cross-validation with Holdout

```{r dt-rf-cv2-r5kf8_a}
dt.rf_cv2_r5kf8_a <- train(dt.formula, data = holdout_age_groups.training, method = "rf", prox= TRUE, trControl = cv.train_control_8_5)
```

```{r dt-rf-cv2-r5kf8_a-train_results}
dt.rf_cv2_r5kf8_a_finalModel <- dt.rf_cv2_r5kf8_a$finalModel
dt_sum.rf_cv2_r5kf8_a <- summary(dt.rf_cv2_r5kf8_a)
#
dt.rf_cv2_r5kf8_a_finalModel$forest[11]
dt.rf_cv2_r5kf8_a_finalModel$forest[12]
#
plot(dt.rf_cv2_r5kf8_a)
plot(dt.rf_cv2_r5kf8_a_finalModel)
```

####Random Forest Cross-validation with Holdout testing

```{r dt-rf-cv2-r5kf8_a-test_results}
dt.rf_cv2_r5kf8_a_test <- predict(dt.rf_cv2_r5kf8_a, newdata = holdout_age_groups.testing)
dt.rf_cv2_r5kf8_a_test_prob <- predict(dt.rf_cv2_r5kf8_a, newdata = holdout_age_groups.testing, type = "prob")

dt.rf_cv2_r5kf8_a_test_cm <- confusionMatrix(dt.rf_cv2_r5kf8_a_test, holdout_age_groups.testing$age_group)
kable(dt.rf_cv2_r5kf8_a_test_cm$table)
dt.rf_cv2_r5kf8_a_test_cm$overall[1]
```

The results of this latest Random Forest model appear near identical to that of the first, which applied bootstrapping to the holdout data. Given the similarities between the various Random Forests apart from the one that appears too perfect, it's worth analysing one of these models later when comparing all models.

###Random Forest Leave-one-out

As with the C4.5 decision tree, Leave-one-out Cross-validation is not suitable for this type of model.

###Random Forest  Decision Tree Conclusion

Random forests are good for avoiding over-fitting, so long as you know what you're doing, particularly with regard to cross-validation.

### Decision Tree Conclusion

Given all the data processing and feature selection work, the results from the decision tree have not been a fruitful as one might have hoped. Perhaps further investigation into more fine tuning could help yield better results.


</section>
