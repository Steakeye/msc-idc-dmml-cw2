---
title: "q6"
output: html_document
bibliography: bibliography.ris
csl: ../build/harvard-university-of-westminster.csl
nocite: | 
  @doc:5a52ed11e4b0e3e5a6364700, @doc:5a52ed04e4b0e3e5a63646ff, @doc:5a52ebfbe4b01d3dd55667a6, @doc:5a52e013e4b02942103f7fc1, @doc:5a52de2be4b01d3dd55666a1, @doc:5a52ddf0e4b0e3e5a63645e4, @doc:5a52ddd8e4b02942103f7f9c, @doc:5a52dd8ce4b0eeb35a4a1e8c, @doc:5a52dd7be4b08e15c00cdc97, @doc:5a52dc89e4b02942103f7f82, @doc:5a52dc77e4b02942103f7f81, @doc:5a52ca43e4b02942103f7d57, @doc:5a52c93ce4b02942103f7d43, @doc:5a5192fbe4b02942103f2d47, @doc:5a4f0c37e4b09d5cea02c23b, @doc:5a4edbf5e4b0ac315a55bfc7, @doc:5a4ec6f3e4b020410252c182, @doc:5a4ec56ae4b0062b162abbf0, @doc:5a4ec538e4b0ac315a55beae, @doc:5a4eba0ee4b020410252c08d, @doc:5a4e5a3ae4b09d5cea02a9a9, @doc:5a4db356e4b0ac315a5583fc, @doc:5a4db301e4b0c2a8b20cd934, @doc:5a4db27ee4b0c2a8b20cd927, @doc:5a4da870e4b0c2a8b20cd807, @doc:5a4c434ae4b0204102524624, @doc:5a4c3ba6e4b0ac315a552aac, @doc:5a4c3b69e4b0ac315a552aa8, @doc:5a4c395ce4b0c2a8b20c9406, @doc:5a4c2612e4b01d3b9773f835, @doc:5a4c1f06e4b0c2a8b20c917c, @doc:5a4bd8d3e4b09d5cea022500, @doc:5a4bd7bbe4b0062b162a30a0, @doc:5a4b8c54e4b09d5cea020a62, @doc:5a4b8c05e4b0c2a8b20c5456, @doc:5a4b82e4e4b0062b162a126c, @doc:5a458a17e4b022cc4f774122, @doc:5a458724e4b000cfec287c20, @doc:5a4582c4e4b02eb8a444a227, @doc:5a458258e4b022cc4f773fdd, @doc:5a457f22e4b022cc4f773f51, @doc:5a457cf1e4b06a7ca331608d
---

```{r setup-t6, include=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r deps-t6, echo=FALSE, include=FALSE, message=FALSE}
if(!require(caret)) install.packages("caret", repos = mirrorUrl)
if(!require(ROCR)) install.packages("ROCR", repos = mirrorUrl)
#
library("caret")
library("ROCR")
```

<section>

#Task 6: Measure Performance

##Premise

>>>For each type of classifier calculate and display the following performance related metrics in R. Use the library library(ROCR)
 1 Confusion matrix
 2 Precision vs. Recall
 3 Accuracy estimation
 4 ROC (receiver operating characteristic curve)
 5 RAUC (receiver under the curve area)

<!--
• Confusion matrix estimation 6
• Precision vs. Recall estimation 4
• Accuracy estimation 4
• ROC(receiver operating characteristic curve) plot 3
• RAUC (receiver under the curve area) plot 3
-->

##Confusion matrix comparison

Every model has had a confusion matrix displayed within its respective section; they are not presented together as the following graphs do a better job of indicating to the human eye which models the most successful. The confusion matrix was instrumental and selecting which models were best in their field but the other metric provide a better way to convey performance at a glance, visualising with 2 dimensions the nature of the models.

##Precision & Recall comparison

>>>The precision-recall plot is a model-wide evaluation measure that is based on two basic evaluation measures – recall and precision. Recall is a performance measure of the whole positive part of a dataset, whereas precision is a performance measure of positive predictions. [@doc:5a52ebfbe4b01d3dd55667a6]

Precision is the measure of result confidence, while recall the measure true positive results are returned. The precision-recall curve shows the tradeoff between precision and recall at different thresholds. 

###C4.5 Decision Tree candidate, 8 folds & 5 repetitions

```{r dt-c4_5-p&r}
pred_dt.c4_5_highest_probs <- apply(dt.c4_5_j48_r5kf8_a_test_prob, 1, "max") 
pred_dt.c4_5_correct <- dt.c4_5_j48_r5kf8_a_test == holdout_age_groups.testing$age_group

pred_dt.c4_5 <- prediction(pred_dt.c4_5_highest_probs, pred_dt.c4_5_correct)

## precision/recall curve (x-axis: recall, y-axis: precision)
pr_dt.c4_5 <- performance(pred_dt.c4_5, "prec", "rec")
plot(pr_dt.c4_5)
```

This is a strange line but appears to suggest that the precision of this model drops off dramatically as there's a lot of overlap between false and true positives.

###C4.5 Decision Tree candidate, 13 folds & 8 repetitions

```{r dt-c4_5-r8kf13-p&r}
pred_dt.c4_5_r8kf13_highest_probs <- apply(dt.c4_5_j48_r8kf13_a_test_prob, 1, "max") 
pred_dt.c4_5_r8kf13_correct <- dt.c4_5_j48_r8kf13_a_test == holdout_age_groups.testing$age_group

pred_dt.c4_5_r8kf13 <- prediction(pred_dt.c4_5_r8kf13_highest_probs, pred_dt.c4_5_r8kf13_correct)

## precision/recall curve (x-axis: recall, y-axis: precision)
pr_dt.c4_5_r8kf13 <- performance(pred_dt.c4_5_r8kf13, "prec", "rec")
plot(pr_dt.c4_5_r8kf13)
```

This line suggests is prettier linear relationship between precision & recall, which is at least better than the previous graph but a better line would remain higher on the _y_ only dropping off towards _1.0_ on the _y_.

###Random Forest Decision Tree repeated k-folds candidate (overfit)

```{r dt-rf-p&r}
pred_dt.rf_highest_probs <- apply(dt.rf_cv_r5kf8_a_test_prob, 1, "max") 
pred_dt.rf_correct <- dt.rf_cv_r5kf8_a_test == holdout_age_groups.testing$age_group

pred_dt.rf <- prediction(pred_dt.rf_highest_probs, pred_dt.correct)

## precision/recall curve (x-axis: recall, y-axis: precision)
pr_dt.rf <- performance(pred_dt.rf, "prec", "rec")
plot(pr_dt.rf)
```

This is the over-fit example which show an example of what the line should be like if precision of classification stayed more or less constant as recall scales.

###Random Forest Decision Tree holdout candidate 

```{r dt-hrf2_h-p&r}
#dt.hrf2_h_a_test_prob
pred_dt.hrf2_h_highest_probs <- apply(dt.hrf2_h_a_test_prob$predictions, 1, "max") 
pred_dt.hrf2_h_correct <- dt.hrf2_h_a_test$predictions == holdout_age_groups.testing$age_group

pred_dt.hrf2_h <- prediction(pred_dt.hrf2_h_highest_probs, pred_dt.hrf2_h_correct)

## precision/recall curve (x-axis: recall, y-axis: precision)
pr_dt.hrf2_h <- performance(pred_dt.hrf2_h, "prec", "rec")
plot(pr_dt.hrf2_h)
```

This line show a saw-tooth pattern that is apparently common in this sort of graph; the slope is approximately the same as the better C4.5 line though the area under the line might well be higher, indicating slightly better performance.

###Naive Bayes holdout candidate

```{r dt-nb_h-p&r}
pred_nb.h_highest_probs <- apply(nb.h_a_test_prob$posterior, 1, "max") 
pred_nb.h_correct <- nb.h_a_test_prob$class == holdout_age_groups.testing$age_group

pred_nb.h <- prediction(pred_nb.h_highest_probs, pred_nb.h_correct)

## precision/recall curve (x-axis: recall, y-axis: precision)
pr_nb.h <- performance(pred_nb.h, "prec", "rec")
plot(pr_nb.h)
```

This graph is interesting as it suggests an initially good rate of success which literally tails off. 

###Naive Bayes repeat k-folds cross-validation candidate

```{r dt-nb_r5kf8-p&r}
pred_nb.r5kf8_highest_probs <- apply(nb.r5kf8_a_test_prob, 1, "max") 
pred_nb.r5kf8_correct <- nb.r5kf8_a_test == holdout_age_groups.testing$age_group

pred_nb.r5kf8 <- prediction(pred_nb.r5kf8_highest_probs, pred_nb.r5kf8_correct)

## precision/recall curve (x-axis: recall, y-axis: precision)
pr_nb.r5kf8 <- performance(pred_dt.nb_r5kf8, "prec", "rec")
plot(pr_nb.r5kf8)
```

When comparing this line to the line of the previous graph, it's possible to see how cross validation manages to improve the line, increasing the area below the line ever so slightly.

###KNN candidates

####KNN holdout candidate

```{r knn-h-p&r}
pred_knn.h_highest_probs <- apply(knn.h_a_test_prob, 1, "max") 
pred_knn.h_correct <- knn.h_a_test == holdout_age_groups.testing$age_group

pred_knn.h <- prediction(pred_knn.h_highest_probs, pred_knn.h_correct)

## precision/recall curve (x-axis: recall, y-axis: precision)
pr_knn.h <- performance(pred_knn.h, "prec", "rec")
plot(pr_knn.h)
```

This line would appear to suggest slightly worse than a linear, negative coefficient; the highest precision value is only about _0.75_, so this doesn't suggest good results where precision is concerned.

####KNN repeated k-folds cross-validation candidate

```{r knn-r5kf8-p&r}
pred_knn.r5kf8_highest_probs <- apply(knn.r5kf8_a_test_prob, 1, "max") 
pred_knn.r5kf8_correct <- knn.r5kf8_a_test == holdout_age_groups.testing$age_group

pred_knn.r5kf8 <- prediction(pred_knn.r5kf8_highest_probs, pred_knn.r5kf8_correct)

## precision/recall curve (x-axis: recall, y-axis: precision)
pr_knn.r5kf8 <- performance(pred_knn.r5kf8, "prec", "rec")
plot(pr_knn.r5kf8)
```

When compared to the previous KNN PRC, this supports the recommendation of cross-validation to improve precision; this also manifests in an increase of the area under the line.

##Accuracy estimation comparison

The following graphs represent the overall accuracy of the model; like with all these graphs, the area under the line is important with the larger volume being preferred.

###C4.5 Decision Tree candidate

```{r dt-c4_5-acc}
acc_dt.c4_5 <- performance(pred_dt.c4_5, "acc")
plot(acc_dt.c4_5)
```

###Random Forest Decision Tree k-folds candidate (overfit)

```{r dt-rf-acc}
acc_dt.rf <- performance(pred_dt.rf, "acc")
plot(acc_dt.rf)
```

###Random Forest Decision Tree holdout candidate

```{r dt-hrf2_h-acc}
acc_dt.hrf2_h <- performance(pred_dt.hrf2_h, "acc")
plot(acc_dt.hrf2_h)
```

###Naive Bayes holdout candidate

```{r nb-h-acc}
acc_nb.h <- performance(pred_nb.h, "acc")
plot(acc_nb.h)
```

###Naive Bayes repeated k-folds cross-validation candidate

```{r nb-r5kf8-acc}
acc_nb.r5kf8 <- performance(pred_nb.r5kf8, "acc")
plot(acc_nb.r5kf8)
```
###KNN holdout candidate

```{r knn-h-acc}
acc_knn.h <- performance(pred_knn.h, "acc")
plot(acc_knn.h)
```

###KNN repeated k-folds cross-validation candidate

```{r knn-r5kf8-acc}
acc_knn.r5kf8 <- performance(pred_knn.r5kf8, "acc")
plot(acc_knn.r5kf8)
```

##ROC comparison

The Receiver operating characteristic compares the rate of true positives with false positives; in graph form true positive rate is on the _y_ axis, while _x_ maps the false positive rate. The better the model is at performing classification the larger the area under the line and the more 'rectangular' in shape, with the best performing results being were the line is closest to the top left corner [@doc:5a52ed11e4b0e3e5a6364700]. The better the model performs, the higher the true positive relative to the false positive.

In the following ROC graphs, a red line indicates what model performance would be like were a model only have a 50% change of correctly classifying a data instance. The yellow line is the relationship between true-positive-rate (TPR) and false-positive-rate (FPR).

###C4.5 Decision Tree candidate

```{r dt-c4_5-roc}
roc_dt.c4_5 = performance(pred_dt.c4_5, measure="tpr", x.measure="fpr")
plot(roc_dt.c4_5, col="orange", lwd=2) 
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=2)
```

This doesn't seem to be a particularly good result, though it is somewhat better than a 50:50 chance of correct classification, with peak performance easily identifiable; the model has the best rate of performance around the middle of the line where there is the best balance between the TPR and the coefficient with FPR, with TPR at over 0.6 and about 50% better than the FPR. After the peak point, the graph suggests accuracy goes down.

###Random Forest Decision Tree k-folds candidate (overfit)

```{r dt-rf-roc}
roc_dt.rf = performance(pred_dt.rf, measure="tpr", x.measure="fpr")
plot(roc_dt.rf, col="orange", lwd=2) 
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=2)
```

This graph is a near perfect example, which has been included to demonstrate overfitting; this was because the test data was used as part of the training data. The yellow line draws a blocky, rectangular shape indicated a high rate of successful classification; if this was not the result of overfitting it would be an excellent result. 

###Random Forest Decision Tree holdout candidate

```{r dt-hrf2_h-roc}
roc_dt.hrf2_h = performance(pred_dt.hrf2_h, measure="tpr", x.measure="fpr")
plot(roc_dt.hrf2_h, col="orange", lwd=2) 
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=2)
```

This is a far more conservative result but it is at least, markedly better than the C4.5 decision tree. What's worth doing is comparing this result to the Naive Bayes graphs below, which happen to have similar looking lines.

###Naive Bayes holdout candidate

```{r nb-h-roc}
roc_nb.h = performance(pred_nb.h, measure="tpr", x.measure="fpr")
plot(roc_nb.h, col="orange", lwd=2) 
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=2)
```

###Naive Bayes repeated k-folds cross-validation candidate

```{r nb-r5kf8-roc}
roc_nb.r5kf8 = performance(pred_nb.r5kf8, measure="tpr", x.measure="fpr")
plot(roc_nb.r5kf8, col="orange", lwd=2) 
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=2)
```

###KNN holdout candidate

```{r knn-h-roc}
roc_knn.h = performance(pred_knn.h, measure="tpr", x.measure="fpr")
plot(roc_knn.h, col="orange", lwd=2) 
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=2)
```

###KNN repeated k-folds cross-validation candidate

```{r knn-r5kf8-roc}
roc_knn.r5kf8 = performance(pred_knn.r5kf8, measure="tpr", x.measure="fpr")
plot(roc_knn.r5kf8, col="orange", lwd=2) 
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=2)
```


##RAUC comparison

The following values essentially represent the volume under the lines for the corresponding ROC curves, thus reducing the performance into one number between _0_ and _1_, where _1_ would indicate perfect classification, _0_ would represent incorrect classification 100% of the time and the red line on the ROC graphs above would essentially resolve to a value of _0.5_. It can be considered another form of accuracy indicator in this way [@doc:5a52ed04e4b0e3e5a63646ff].

###C4.5 Decision Tree candidate

```{r dt-c4_5-auc}
auc_dt.c4_5 = performance(pred_dt.c4_5, "auc")
print(paste(c(auc_dt.c4_5@y.name, auc_dt.c4_5@y.values)), quote=FALSE)
```

###Random Forest Decision Tree k-folds candidate (overfit)

```{r dt-rf-auc}
auc_dt.rf = performance(pred_dt.rf, "auc")
print(paste(c(auc_dt.rf@y.name, auc_dt.rf@y.values)), quote=FALSE)
```

###Random Forest Decision Tree holdout candidate

```{r dt-hrf2_h-auc}
auc_dt.hrf2_h = performance(pred_dt.hrf2_h, "auc")
print(paste(c(auc_dt.hrf2_h@y.name, auc_dt.hrf2_h@y.values)), quote=FALSE)
```

###Naive Bayes holdout candidate

```{r nb-h-auc}
auc_nb.h = performance(pred_nb.h, "auc")
print(paste(c(auc_nb.h@y.name, auc_nb.h@y.values)), quote=FALSE)
```

###Naive Bayes repeated k-folds cross-validation candidate

```{r nb-r5kf8-auc}
auc_nb.r5kf8 = performance(pred_nb.r5kf8, "auc")
print(paste(c(auc_nb.r5kf8@y.name, auc_nb.r5kf8@y.values)), quote=FALSE)
```

###KNN holdout candidate

```{r knn-h-auc}
auc_knn.h = performance(pred_knn.h, "auc")
print(paste(c(auc_knn.h@y.name, auc_knn.h@y.values)), quote=FALSE)
```

###KNN repeated k-folds cross-validation candidate

```{r knn-r5kf8-auc}
auc_knn.r5kf8 = performance(pred_knn.r5kf8, "auc")
print(paste(c(auc_knn.r5kf8@y.name, auc_knn.r5kf8@y.values)), quote=FALSE)
```

##Conclusion

If the over-fit Random Forest wasn't considered over-fit, then it would be the recommended model; further experimentation would probably demonstrate how it has been too tightly shaped to the test dataset to deliver similar results for fresh Abalone data.

If one metric were to be used to generally determine the which were the better performing models then it would be the RAUC, in which case the cross-validated KNN model comes out on top _(ROAC: 0.722)_ while the Random Forest with holdout training comes in second _(ROAC: 0.701)_; both of these values outperform the best Naive Bayes model _(ROAC: 0.695)_ though not by much, so further investigation into tuning would probably be a good idea. The good thing about these models, Random Forest and KNN is that in their own way they are fast, so while the classification performance could be improved, these two type of models would be recommended over the C4.5 decision tree.

Whichever model is used, repeated cross-validation has benefit, and the leave-one-out method doesn't seem worth the effort.

##Further work

With more time, extra analysis would be carried out to determine the accuracy of classification for each particular age group. At the moment the _old_ age group is generally, very poorly classified, so attempts to improve that particular classification would be beneficial to overall model performance.

Experimentation with Random Forests against incomplete data would be interesting with regard to exploring the usefulness of this type of model over other types. More study of Random Forests and overfitting is required to get a better understanding of whether or not the best performing model as actually overfit or not; a quick update, to look at out-of-bag prediction suggests that the results are good; this will have to be continued...


</section>
