---
title: "q6"
output: html_document
bibliography: bibliography.ris
---


```{r setup-t6, include=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r deps-t6, echo=FALSE, include=FALSE, message=FALSE}
if(!require(caret)) install.packages("caret", repos = mirrorUrl)
if(!require(ROCR)) install.packages("ROCR", repos = mirrorUrl)
#
library("caret")
library("ROCR")
```

<section>

#Task 6: Measure Performance

##Premise

>>>For each type of classifier calculate and display the following performance related metrics in R. Use the library library(ROCR)
 1 Confusion matrix
 2 Precision vs. Recall
 3 Accuracy estimation
 4 ROC (receiver operating characteristic curve)
 5 RAUC (receiver under the curve area)

<!--
• Confusion matrix estimation 6
• Precision vs. Recall estimation 4
• Accuracy estimation 4
• ROC(receiver operating characteristic curve) plot 3
• RAUC (receiver under the curve area) plot 3
-->

##Confusion matrix comparison

Every model has had a confusion matrix displayed within its respective section; they are not presented together as the following graphs do a better job of indicating to the human eye which models the most successful. The confusion matrix was instrumental and selecting which models were best in their field but the other metric provide a better way to convey performance at a glance, visualsing with 2 dimensions the nature of the models.

##Precision & Recall comparison

>>>Precision-Recall is a useful measure of success of prediction when the classes are very imbalanced. In information retrieval, precision is a measure of result relevancy, while recall is a measure of how many truly relevant results are returned.
The precision-recall curve shows the tradeoff between precision and recall for different threshold. A high area under the curve represents both high recall and high precision, where high precision relates to a low false positive rate, and high recall relates to a low false negative rate. High scores for both show that the classifier is returning accurate results (high precision), as well as returning a majority of all positive results (high recall).
A system with high recall but low precision returns many results, but most of its predicted labels are incorrect when compared to the training labels. A system with high precision but low recall is just the opposite, returning very few results, but most of its predicted labels are correct when compared to the training labels. An ideal system with high precision and high recall will return many results, with all results labeled correctly.

- [TODO: cite!]

```{r f-score-func}
 f_score <- function (prec, rec)
{
  return(2 * prec * rec / (prec + rec))
}
```

###C4.5 Decision Tree candidate, 8 folds & 5 repetitions

```{r dt-c4_5-p&r}
pred_dt.c4_5_highest_probs <- apply(dt.c4_5_j48_r5kf8_a_test_prob, 1, "max") 
pred_dt.c4_5_correct <- dt.c4_5_j48_r5kf8_a_test == holdout_age_groups.testing$age_group

pred_dt.c4_5 <- prediction(pred_dt.c4_5_highest_probs, pred_dt.c4_5_correct)

## precision/recall curve (x-axis: recall, y-axis: precision)
pr_dt.c4_5 <- performance(pred_dt.c4_5, "prec", "rec")
plot(pr_dt.c4_5)
```

This is a strange line but appears to suggest that the precision of this model drops off dramatically as there's a lot of overlap between false and true positives.

###C4.5 Decision Tree candidate, 13 folds & 8 repetitions

```{r dt-c4_5-r8kf13-p&r}
pred_dt.c4_5_r8kf13_highest_probs <- apply(dt.c4_5_j48_r8kf13_a_test_prob, 1, "max") 
pred_dt.c4_5_r8kf13_correct <- dt.c4_5_j48_r8kf13_a_test == holdout_age_groups.testing$age_group

pred_dt.c4_5_r8kf13 <- prediction(pred_dt.c4_5_r8kf13_highest_probs, pred_dt.c4_5_r8kf13_correct)

## precision/recall curve (x-axis: recall, y-axis: precision)
pr_dt.c4_5_r8kf13 <- performance(pred_dt.c4_5_r8kf13, "prec", "rec")
plot(pr_dt.c4_5_r8kf13)
```

This line suggests is prettier linear relationship between pecision & recall, which is at least better than the previous graph but a better line would remain higher on the _y_ only dropping off towards _1.0_ on the _y_.

###Random Forest Decision Tree repeated k-folds candidate (overfit)

```{r dt-rf-p&r}
pred_dt.rf_highest_probs <- apply(dt.rf_cv_r5kf8_a_test_prob, 1, "max") 
pred_dt.rf_correct <- dt.rf_cv_r5kf8_a_test == holdout_age_groups.testing$age_group

pred_dt.rf <- prediction(pred_dt.rf_highest_probs, pred_dt.correct)

## precision/recall curve (x-axis: recall, y-axis: precision)
pr_dt.rf <- performance(pred_dt.rf, "prec", "rec")
plot(pr_dt.rf)
```

This is the overfit example which show an example of what the line should be like if precision of classification stayed more or less constant as recall scales.

###Random Forest Decision Tree holdout candidate 

```{r dt-hrf2_h-p&r}
#dt.hrf2_h_a_test_prob
pred_dt.hrf2_h_highest_probs <- apply(dt.hrf2_h_a_test_prob$predictions, 1, "max") 
pred_dt.hrf2_h_correct <- dt.hrf2_h_a_test$predictions == holdout_age_groups.testing$age_group

pred_dt.hrf2_h <- prediction(pred_dt.hrf2_h_highest_probs, pred_dt.hrf2_h_correct)

## precision/recall curve (x-axis: recall, y-axis: precision)
pr_dt.hrf2_h <- performance(pred_dt.hrf2_h, "prec", "rec")
plot(pr_dt.hrf2_h)
```

This line show a saw-tooth pattern that is apparently common in this sort of graph; the slope is approximately the same as the better C4.5 line though the area under the line might well be higher, indicating slightly better performance.

###Naive Bayes holdout candidate

```{r dt-nb_h-p&r}
pred_nb.h_highest_probs <- apply(nb.h_a_test_prob$posterior, 1, "max") 
pred_nb.h_correct <- nb.h_a_test_prob$class == holdout_age_groups.testing$age_group

pred_nb.h <- prediction(pred_nb.h_highest_probs, pred_nb.h_correct)

## precision/recall curve (x-axis: recall, y-axis: precision)
pr_nb.h <- performance(pred_nb.h, "prec", "rec")
plot(pr_nb.h)
```

This grapghi is insteresting as it suggests an initially good rate of success which literally tails off. 

###Naive Bayes repeat k-folds cross-validation candidate

```{r dt-nb_r5kf8-p&r}
pred_nb.r5kf8_highest_probs <- apply(nb.r5kf8_a_test_prob, 1, "max") 
pred_nb.r5kf8_correct <- nb.r5kf8_a_test == holdout_age_groups.testing$age_group

pred_nb.r5kf8 <- prediction(pred_nb.r5kf8_highest_probs, pred_nb.r5kf8_correct)

## precision/recall curve (x-axis: recall, y-axis: precision)
pr_nb.r5kf8 <- performance(pred_dt.nb_r5kf8, "prec", "rec")
plot(pr_nb.r5kf8)
```

When comparing this line to the line onf the previous graph, it's possible to see how cross validation manages to improve the line, increasing the area below the line ever so slightly.

###KNN candidates

####KNN holdout candidate

```{r knn-h-p&r}
pred_knn.h_highest_probs <- apply(knn.h_a_test_prob, 1, "max") 
pred_knn.h_correct <- knn.h_a_test == holdout_age_groups.testing$age_group

pred_knn.h <- prediction(pred_knn.h_highest_probs, pred_knn.h_correct)

## precision/recall curve (x-axis: recall, y-axis: precision)
pr_knn.h <- performance(pred_knn.h, "prec", "rec")
plot(pr_knn.h)
```

This line would appear to suggest slightly worse than a linear, negative coefficient; the highest precision value is only about _0.75_, so this doesn't suggest good results where precision is concerned.

####KNN repeated k-folds cross-validation candidate

```{r knn-r5kf8-p&r}
pred_knn.r5kf8_highest_probs <- apply(knn.r5kf8_a_test_prob, 1, "max") 
pred_knn.r5kf8_correct <- knn.r5kf8_a_test == holdout_age_groups.testing$age_group

pred_knn.r5kf8 <- prediction(pred_knn.r5kf8_highest_probs, pred_knn.r5kf8_correct)

## precision/recall curve (x-axis: recall, y-axis: precision)
pr_knn.r5kf8 <- performance(pred_knn.r5kf8, "prec", "rec")
plot(pr_knn.r5kf8)
```

When compared to the previous KNN PRC, this supports the recommendation of cross-validation to improve precision; this also manifests in an increase of the area under the line.

##Accuracy estimation comparison

The following graphs represent the overall accuracy of the model; like with all thse graphs, the area under the line is important with the larger volume being preferred.

###C4.5 Decision Tree candidate

```{r dt-c4_5-acc}
acc_dt.c4_5 <- performance(pred_dt.c4_5, "acc")
plot(acc_dt.c4_5)
```

###Random Forest Decision Tree k-folds candidate (overfit)

```{r dt-rf-acc}
acc_dt.rf <- performance(pred_dt.rf, "acc")
plot(acc_dt.rf)
```

###Random Forest Decision Tree holdout candidate

```{r dt-hrf2_h-acc}
acc_dt.hrf2_h <- performance(pred_dt.hrf2_h, "acc")
plot(acc_dt.hrf2_h)
```

###Naive Bayes holdout candidate

```{r nb-h-acc}
acc_nb.h <- performance(pred_nb.h, "acc")
plot(acc_nb.h)
```

###Naive Bayes repeated k-folds cross-validation candidate

```{r nb-r5kf8-acc}
acc_nb.r5kf8 <- performance(pred_nb.r5kf8, "acc")
plot(acc_nb.r5kf8)
```
###KNN holdout candidate

```{r knn-h-acc}
acc_knn.h <- performance(pred_knn.h, "acc")
plot(acc_knn.h)
```

###KNN repeated k-folds cross-validation candidate

```{r knn-r5kf8-acc}
acc_knn.r5kf8 <- performance(pred_knn.r5kf8, "acc")
plot(acc_knn.r5kf8)
```

##ROC comparison

The Receiver operating characteristic compares the rate of true positives with false positives; in graph form true positive rate is on the _y_ axis, while _x_ maps the false positive rate. The better the model is at performing classification the larger the area under the line and the more 'rectanglar' in shape, with the best performing results being were the line is closest to the top left corner. The better the model performs the higher the true positive relative to the false positive.

In the follwing ROC grpahs, a red line indicates what model performance would be like were a model only have a 50% change of correctly classifying a data instance. The yellow line is the relationship between true-positive-rate (TPR) and false-positive-rate (FPR).

###C4.5 Decision Tree candidate

```{r dt-c4_5-roc}
roc_dt.c4_5 = performance(pred_dt.c4_5, measure="tpr", x.measure="fpr")
plot(roc_dt.c4_5, col="orange", lwd=2) 
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=2)
```

This doesn't seem to be a particularly good result, though it is somewhat better than a 50:50 chance of correct classification, with peak performance easily identifiable; the model has the best rate of performance around the middle of the line where there is the best balance between the TPR and the coefficient with FPR, with TPR at over 0.6 and about 50% better than the FPR. After the peak point, the graph suggests accuracy goes down.

###Random Forest Decision Tree k-folds candidate (overfit)

```{r dt-rf-roc}
roc_dt.rf = performance(pred_dt.rf, measure="tpr", x.measure="fpr")
plot(roc_dt.rf, col="orange", lwd=2) 
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=2)
```

This graph is a near perfect example, which has been included to demonstrate overfitting; this was because the test data was used as part of the training data. The yellow line draws a blocky rectangular shape indicated a high rate of succusful classification; if this was not the result of overfitting it would be an excellent result. 

###Random Forest Decision Tree holdout candidate

```{r dt-hrf2_h-roc}
roc_dt.hrf2_h = performance(pred_dt.hrf2_h, measure="tpr", x.measure="fpr")
plot(roc_dt.hrf2_h, col="orange", lwd=2) 
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=2)
```

This is a far more conservative result but it is at least, markedly better than the C4.5 decision tree. What's worth doing is comparing this reulst to the Naive Bayes graphs below, which happen to have similar looking lines.

###Naive Bayes holdout candidate

```{r nb-h-roc}
roc_nb.h = performance(pred_nb.h, measure="tpr", x.measure="fpr")
plot(roc_nb.h, col="orange", lwd=2) 
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=2)
```

###Naive Bayes repeated k-folds cross-validation candidate

```{r nb-r5kf8-roc}
roc_nb.r5kf8 = performance(pred_nb.r5kf8, measure="tpr", x.measure="fpr")
plot(roc_nb.r5kf8, col="orange", lwd=2) 
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=2)
```

###KNN holdout candidate

```{r knn-h-roc}
roc_knn.h = performance(pred_knn.h, measure="tpr", x.measure="fpr")
plot(roc_knn.h, col="orange", lwd=2) 
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=2)
```

###KNN repeated k-folds cross-validation candidate

```{r knn-r5kf8-roc}
roc_knn.r5kf8 = performance(pred_knn.r5kf8, measure="tpr", x.measure="fpr")
plot(roc_knn.r5kf8, col="orange", lwd=2) 
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=2)
```


##RAUC comparison

The following values essentially represent the volume under the lines for the corresponding ROC curves, thus distiling the performance into one number between _0_ and _1_, where _1_ would indicate perfect classification, _0_ would represent incorrect clasification 100% of the time and the red line on the ROC graphs above would essentially resolve to a value of _0.5_. It can be considered another form of accuracy indicator in this way.

###C4.5 Decision Tree candidate

```{r dt-c4_5-roc}
auc_dt.c4_5 = performance(pred_dt.c4_5, "auc")
print(paste(c(auc_dt.c4_5@y.name, auc_dt.c4_5@y.values)), quote=FALSE)
```

###Random Forest Decision Tree k-folds candidate (overfit)

```{r dt-rf-roc}
auc_dt.rf = performance(pred_dt.rf, "auc")
print(paste(c(auc_dt.rf@y.name, auc_dt.rf@y.values)), quote=FALSE)
```

###Random Forest Decision Tree holdout candidate

```{r dt-hrf2_h-roc}
auc_dt.hrf2_h = performance(pred_dt.hrf2_h, "auc")
print(paste(c(auc_dt.hrf2_h@y.name, auc_dt.hrf2_h@y.values)), quote=FALSE)
```

###Naive Bayes holdout candidate

```{r nb-h-roc}
auc_nb.h = performance(pred_nb.h, "auc")
print(paste(c(auc_nb.h@y.name, auc_nb.h@y.values)), quote=FALSE)
```

###Naive Bayes repeated k-folds cross-validation candidate

```{r nb-r5kf8-roc}
auc_nb.r5kf8 = performance(pred_nb.r5kf8, "auc")
print(paste(c(auc_nb.r5kf8@y.name, auc_nb.r5kf8@y.values)), quote=FALSE)
```

###KNN holdout candidate

```{r knn-h-roc}
auc_knn.h = performance(pred_knn.h, "auc")
print(paste(c(auc_knn.h@y.name, auc_knn.h@y.values)), quote=FALSE)
```

###KNN repeated k-folds cross-validation candidate

```{r knn-r5kf8-roc}
auc_knn.r5kf8 = performance(pred_knn.r5kf8, "auc")
print(paste(c(auc_knn.r5kf8@y.name, auc_knn.r5kf8@y.values)), quote=FALSE)
```

##Conclusion

If the overfit Random Forest wasn't considered overfit, then it would be the recommended model; further experimentation would probably demonstrate how it has been too tightly shaped to the test dataset to deliver similar results for fresh Abalone data.

If one metric were to be used to generally determine the which were the better performing models then it would be the RAUC, in which case the cross-validated KNN model comes out on top _(ROAC: 0.722)_ while the Random Forest with holdout training comes in second _(ROAC: 0.701)_; both of these values outperform the best Naive Bayes model _(ROAC: 0.695)_ though not by much, so further investigation into tuning would probably be a good idea. The good thing about these models, Random Forest and KNN is that in their own way they are fast, so while the classification performance could be improved, these two type of models would be recommendedover the C4.5 decision tree.

</section>
