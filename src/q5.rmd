---
title: "q5"
output: html_document
bibliography: bibliography.ris
csl: ../build/harvard-university-of-westminster.csl
nocite: | 
  @doc:5a52ed11e4b0e3e5a6364700, @doc:5a52ed04e4b0e3e5a63646ff, @doc:5a52ebfbe4b01d3dd55667a6, @doc:5a52e013e4b02942103f7fc1, @doc:5a52de2be4b01d3dd55666a1, @doc:5a52ddf0e4b0e3e5a63645e4, @doc:5a52ddd8e4b02942103f7f9c, @doc:5a52dd8ce4b0eeb35a4a1e8c, @doc:5a52dd7be4b08e15c00cdc97, @doc:5a52dc89e4b02942103f7f82, @doc:5a52dc77e4b02942103f7f81, @doc:5a52ca43e4b02942103f7d57, @doc:5a52c93ce4b02942103f7d43, @doc:5a5192fbe4b02942103f2d47, @doc:5a4f0c37e4b09d5cea02c23b, @doc:5a4edbf5e4b0ac315a55bfc7, @doc:5a4ec6f3e4b020410252c182, @doc:5a4ec56ae4b0062b162abbf0, @doc:5a4ec538e4b0ac315a55beae, @doc:5a4eba0ee4b020410252c08d, @doc:5a4e5a3ae4b09d5cea02a9a9, @doc:5a4db356e4b0ac315a5583fc, @doc:5a4db301e4b0c2a8b20cd934, @doc:5a4db27ee4b0c2a8b20cd927, @doc:5a4da870e4b0c2a8b20cd807, @doc:5a4c434ae4b0204102524624, @doc:5a4c3ba6e4b0ac315a552aac, @doc:5a4c3b69e4b0ac315a552aa8, @doc:5a4c395ce4b0c2a8b20c9406, @doc:5a4c2612e4b01d3b9773f835, @doc:5a4c1f06e4b0c2a8b20c917c, @doc:5a4bd8d3e4b09d5cea022500, @doc:5a4bd7bbe4b0062b162a30a0, @doc:5a4b8c54e4b09d5cea020a62, @doc:5a4b8c05e4b0c2a8b20c5456, @doc:5a4b82e4e4b0062b162a126c, @doc:5a458a17e4b022cc4f774122, @doc:5a458724e4b000cfec287c20, @doc:5a4582c4e4b02eb8a444a227, @doc:5a458258e4b022cc4f773fdd, @doc:5a457f22e4b022cc4f773f51, @doc:5a457cf1e4b06a7ca331608d
---

```{r setup-t5, include=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r deps-t5, echo=FALSE, include=FALSE, message=FALSE}
```

<section>

#Task 5: Build Train and Test a K-NN type Classifier

##Premise

>You need to construct, train and test K-NN type classifier in R. Train and test your K-NN classifier using the training and test sets generated based on the methods tried as part of the 2nd Task.

<!--
• Building of K-NN type classifier in R 4
• Training of K-NN type classifier in R 5
• Testing of K-NN type classifier in R 6
-->

##KNN modelling

K-Nearest=Neigbours looks to classify new data based on it's dimensional proximity to an already classified node or group of nodes [@doc:5a52ddd8e4b02942103f7f9c, @doc:5a52ddf0e4b0e3e5a63645e4]. This use of dimensional distance allows it to be used for regression but every new classification needs to be computed against the relative distance to the instances within the training dataset; this makes it fast to train because a lot of the computation is deferred but then using this method to classify is more expensive because every new classification is calculated at run-time instead of relying on an established rule-set.

###KNN with holdout

```{r knn-h_a}
knn.h_a <- train(dt.formula, data=holdout_age_groups.training, method = "knn")
```

```{r knn-h_a-summary}
plot(knn.h_a)
```

####KNN with holdout, testing

```{r knn-h_a_test_results}
knn.h_a_test <- predict(knn.h_a, holdout_age_groups.testing)
knn.h_a_test_prob <- predict(knn.h_a, holdout_age_groups.testing, type = "prob")
# summarize results
knn.h_a_test_cm <- confusionMatrix(knn.h_a_test, holdout_age_groups.testing$age_group)
knn.h_a_test_cm$table
```

###KNN with Repeated K-folds Cross-validation

```{r knn-r5kf8_a}
knn.r5kf8_a <- train(dt.formula, data=abalone_data_cleansed_age_groups, trControl = cv.train_control_8_5, method = "knn")
```

```{r knn-r5kf8_a-summary}
plot(knn.r5kf8_a)
```

####KNN with Repeated K-folds Cross-validation, testing

```{r knn-r5kf8_a_test_results}
knn.r5kf8_a_test <- predict(knn.r5kf8_a, holdout_age_groups.testing)
knn.r5kf8_a_test_prob <- predict(knn.r5kf8_a, holdout_age_groups.testing, type = "prob")
# summarize results
knn.r5kf8_a_test_cm <- confusionMatrix(knn.r5kf8_a_test, holdout_age_groups.testing$age_group)
knn.r5kf8_a_test_cm$table
```

###Naive Bayes with Leave-one-out Cross-validation

```{r knn-loo_a}
knn.loo_a <- train(dt.formula, data=abalone_data_cleansed_age_groups, trControl = loocv.train_control, method="knn")
```

```{r knn-loo_a-summary}
plot(knn.loo_a)
```

####Naive Bayes with Leave-one-out, training results

```{r knn-loo_a_test_results}
knn.loo_a_test <- predict(knn.loo_a, holdout_age_groups.testing)
# summarize results
knn.loo_a_test_cm <- confusionMatrix(knn.loo_a_test, holdout_age_groups.testing$age_group)
knn.loo_a_test_cm$table
```



###Accuracy comparison of KNN models

```{r knn-comparisons}
knn.h_a_test_cm$overall[1]
knn.r5kf8_a_test_cm$overall[1]
knn.loo_a_test_cm$overall[1]
```

###KNN model conclusion

Once again, sufficient cross-validation appears to be about as good as using the leave-one-out technique for improving the accuracy of the model but cheaper to train than leave-one-out. In this case, there's quite a marked jump in improvement between the holdout version and the other two forms of training. The holdout KNN is only marginally more accurate than the Naive Bayes while the k-folds and leave-one-out methods are more than 10% more accurate. The results suggest that KNN could be the most suitable model for age group classification of Abalone.


</section>
