---
title: "q4"
output: html_document
bibliography: bibliography.ris
csl: ../build/harvard-university-of-westminster.csl
nocite: | 
  @doc:5a52e013e4b02942103f7fc1, @doc:5a52de2be4b01d3dd55666a1, @doc:5a52ddf0e4b0e3e5a63645e4, @doc:5a52ddd8e4b02942103f7f9c, @doc:5a52dd8ce4b0eeb35a4a1e8c, @doc:5a52dd7be4b08e15c00cdc97, @doc:5a52dc89e4b02942103f7f82, @doc:5a52dc77e4b02942103f7f81, @doc:5a52ca43e4b02942103f7d57, @doc:5a52c93ce4b02942103f7d43, @doc:5a5192fbe4b02942103f2d47, @doc:5a4f0c37e4b09d5cea02c23b, @doc:5a4edbf5e4b0ac315a55bfc7, @doc:5a4ec6f3e4b020410252c182, @doc:5a4ec56ae4b0062b162abbf0, @doc:5a4ec538e4b0ac315a55beae, @doc:5a4eba0ee4b020410252c08d, @doc:5a4e5a3ae4b09d5cea02a9a9, @doc:5a4db356e4b0ac315a5583fc, @doc:5a4db301e4b0c2a8b20cd934, @doc:5a4db27ee4b0c2a8b20cd927, @doc:5a4da870e4b0c2a8b20cd807, @doc:5a4c434ae4b0204102524624, @doc:5a4c3ba6e4b0ac315a552aac, @doc:5a4c3b69e4b0ac315a552aa8, @doc:5a4c395ce4b0c2a8b20c9406, @doc:5a4c2612e4b01d3b9773f835, @doc:5a4c1f06e4b0c2a8b20c917c, @doc:5a4bd8d3e4b09d5cea022500, @doc:5a4bd7bbe4b0062b162a30a0, @doc:5a4b8c54e4b09d5cea020a62, @doc:5a4b8c05e4b0c2a8b20c5456, @doc:5a4b82e4e4b0062b162a126c, @doc:5a458a17e4b022cc4f774122, @doc:5a458724e4b000cfec287c20, @doc:5a4582c4e4b02eb8a444a227, @doc:5a458258e4b022cc4f773fdd, @doc:5a457f22e4b022cc4f773f51, @doc:5a457cf1e4b06a7ca331608d
---

```{r setup-t4, include=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r deps-t4, echo=FALSE, include=FALSE, message=FALSE}
if(!require(caret)) install.packages("caret", repos = mirrorUrl)
if(!require(klaR)) install.packages("klaR", repos = mirrorUrl)
if(!require(rpart.plot)) install.packages("rpart.plot", repos = mirrorUrl)

#
library("caret")
library("klaR")
library("rpart.plot")
```

<section>

#Task 4: Build Train and Test a Naïve Bayes type Classifier

##Premise

>You need to construct, train and test Naïve Bayes type classifier in R. Train and test your Naïve Byes classifier using the training and test sets generated based on the methods tried as part of the 2nd Task.

<!--
• Building of Naïve Bayes type classifier in R 4
• Training of Naïve Bayes type classifier in R 5
• Testing of Naïve Bayes type classifier in R 6
-->

Naive Bayes is a relatively quick and simple probabilistic classifier that is often used as a benchmark [@doc:5a4f0c37e4b09d5cea02c23b] for other forms of classification; if another technique is in some way superior, be that in terms of speed or accuracy, then it's worth sharing. If a proposed algorithm cannot improve on Naive Bayes, then it needs further work, or consigned to only be of use to a very niche problem or set aside completely.

##Naive Bayes training

###Naive Bayes formula and simpler classification

Precedent was set in the previous task to use a specific formula to target key predictors _(weight.shell, height, diameter)_ and classify data instances based on a factor representing simplified age grouping; that precedent applies to the modelling in this chapter and following sections.

###Naive Bayes with holdout

```{r nb-h_a}
nb.h_a <- NaiveBayes(dt.formula, data=holdout_age_groups.training)
```

```{r nb-h_a-summary}
plot(nb.h_a)
```

What's interesting about these Naive Bayes plots is that they suggest the merit of using this techniques to select features that could be used when creating a formula for use with another model. Another aspect worth noting is how the height and diameter probability graphs are extremely similar, which could suggest redundancy, where another feature could have been used as a classifier variable.

####Naive Bayes with holdout, training results

```{r nb-h_a_test_results}
nb.h_a_test <- predict(nb.h_a, holdout_age_groups.testing)
nb.h_a_test_prob <- predict(nb.h_a, holdout_age_groups.testing, type = "prob")
# summarize results
nb.h_a_test_cm <- confusionMatrix(nb.h_a_test$class, holdout_age_groups.testing$age_group)
nb.h_a_test_cm
```

###Naive Bayes with Repeated K-folds Cross-validation

```{r nb-r5kf8_a}
nb.r5kf8_a <- train(dt.formula, data=abalone_data_cleansed_age_groups, trControl = cv.train_control_8_5, method="nb")
```

```{r nb-r5kf8_a-summary}
plot(nb.r5kf8_a)
```

####Naive Bayes with Repeated K-folds, training results

```{r nb-r5kf8_a_test_results}
nb.r5kf8_a_test <- predict(nb.r5kf8_a, holdout_age_groups.testing)
nb.r5kf8_a_test_prob <- predict(nb.r5kf8_a, holdout_age_groups.testing, type = "prob")
# summarize results
nb.r5kf8_a_test_cm <- confusionMatrix(nb.r5kf8_a_test, holdout_age_groups.testing$age_group)
nb.r5kf8_a_test_cm
```

###Naive Bayes with Leave-one-out Cross-validation

```{r nb-loo_a}
nb.loo_a <- train(dt.formula, data=abalone_data_cleansed_age_groups, trControl = loocv.train_control, method="nb")
```

```{r nb-loo_a-summary}
plot(nb.loo_a)
```

####Naive Bayes with Leave-one-out, training results

```{r nb-loo_a_test_results}
nb.loo_a_test <- predict(nb.loo_a, holdout_age_groups.testing)
# summarize results
nb.loo_a_test_cm <- confusionMatrix(nb.loo_a_test, holdout_age_groups.testing$age_group)
nb.loo_a_test_cm
```

###Accuracy comparison of NB models


```{r}
nb.h_a_test_cm$overall[1]
nb.r5kf8_a_test_cm$overall[1]
nb.loo_a_test_cm$overall[1]
```

##Naive Bayes Conclusion

While this technique doesn't produce the strongest results, it's a useful tool alongside other models which can be used for preparatory work in order to inform further work on modelling a classification problem. It's a fast way to do preliminary analysis, which can benefit somewhat from cross-validation but for the Abalone dataset at least, using the leave-one-out method bore no extra benefit over k-folds.

When compared to the decision trees, the Naive Bayes models perform similarly, only marginally worse in some cases; the one classification out of the three agre groups where they underperform by a notable amount is the _'old'_ group; it's not entirely clear why this is but it likely that while the chosen features have a direct correlation to age, the coefficients are not pronounced enough to make classification any better than it is.

</section>
